# Создание MLOps структуры для обучения генеративной языковой модели

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/f11424c0-fa73-4e8e-9ca5-d6a8889fca1a)

## Цель проекта
Основной целью проекта является разработка инфраструктуры для сбора, обработки данных в целях обучения языковой генеративной модели для использования в приложении. Для учебного примера модель должна генерировать по запросу пользователя ежедневный гороскоп по знаку Зодиака. Данные обновляются ежеденевно и модель переобучается на систематической основе. Переобученная модель будет использоваться в приложении.

## Pipeline
Схема обработки данных представлена ниже:

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/afba4ee5-7302-4e80-8dad-d20c5667b8e7)


### Сбор данных
Сбор данных осуществляется с нескольких сайтов один раз в день. Так как структура данных разная, то при сборе данные приводятся к одной схеме. Сбор данных оркеструется Airflow, который размещен на ВМ в облаке. DAG для сбора данных можно посмотреть вот [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/Fin_project/DAGs/DAG_data_collect.py) .

Используется готовая конфигурация Airflow от Yandex с уже установленными провайдерами. После сбора и приведения к единой структуре, данные добавляются в папку raw data в object storage. Данные собираюися в единый csv файл.

<hr>
<details>
  <summary>Технические детали - click to open:</summary>

В основе ВМ с установленной конфинурацией Apache Airflow 2.2.3. Основной используемый оператор ```PythonOperator```. Этот оператор уже есть в установленной конфигурации, поэтому дополнительных настроек не требуется.

Дполнительно нужно установить следующие библиотеки на ВМ:
```
  sudo python3 -m pip install pandas
  sudo python3 -m pip install BeautifulSoup4
  sudo python3 -m pip install lxml
  sudo python3 -m pip install s3fs
```
Для работы с бакетом s3 добавляем в переменные ключ и секрет от соответвующего бакета, где должны храниться собранные данные. Ключ и сектрет были сгенерированы в UI для сервисного аккаунта, который используется при создании ВМ с Airflow.

</details>
<hr>

### Обработка данных для языковой модели
Основная цель обработки - подготовить данные для обучения языковой модели. Этапы обработки:

- удаление пустых строк и дубликатов
- удаление всех знаков кроме пунктуации, текста
- отбор по длине текста (отбираем тексты не меньше 300 знаков и не более 500, чтобы не нрагружать читаталя :))

Скрипт для обработки в виде задания (можно посмотреть [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/Fin_project/Scripts/data_process_pyspark.py)) будет запускаться на временном кластере Spark. Для создания временного кластера, запуска на нем задания и последующего удаления кластера Airflow запускает соответствующий DAG, который можно посмотреть [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/Fin_project/DAGs/DAG_data_process.py). 

После обработки данные сохраняются в object storage в папку processed в формате csv.

<hr>
<details>
  <summary>Технические детали - click to open:</summary>

В основе ВМ с установленной конфинурацией Apache Airflow 2.2.3. Скрипт для обработки данных находится в object storage в папке scripts. Для работы DAG требуется провайдер ```yandex```, который уже есть в предустановленной конфигурации, поэтому дополнительных настроек не требуется.

В переменные airflow нужно добавать:
- ключ и секрет к бакету, где храниться скрипт, данные
- публичный ssh-ключ для создания кластера pyspark
- авторизованный ssh ключ для сервисного аккаунта (при этом у аккаунта должны быть права на создания кластера ```mdb.dataproc.agent```, ```dataproc.agent```, ```dataproc.agent``` и на пользование облаком ```vpc.user```). Ключ создается в UI и сохраняется на ВМ, где развернут Airflow. Путь к ключу указывается в переменных Airflow.

В DAG используется операторы ```DataprocCreateClusterOperator```, ```DataprocCreatePysparkJobOperator```, ```DataprocDeleteClusterOperator```.

</details>
<hr>

### Обучение модели
Для обучения модели я буду использовать библиотеку ```datasphere``` с возможностью запуска задания на разных конфигурациях GPU с локальной или облачной ВМ с использованием ресурсов только на время вычисления. Детальная [инструкция](https://cloud.yandex.ru/ru/docs/datasphere/operations/projects/work-with-jobs) от Yandex. Так как это новая разработка, то у Яндекса нет специального оператора для запуска заданий из Airflow, поэтому я воспользусь SSHOperator.
Скрипт для обучения модели можно посмотреть [здесь](). Скрипт используемый Airflow для запуска задания [здесь](). Для хранения среды и скриптов для задания создаетс отдельная ВМ на Ubuntu.

Результаты обучения ???

Обученная модель сохраняется в репозиторий с моделями (указать какой)

<hr>
<details>
  <summary>Технические детали - click to open:</summary>
  
В основе ВМ с установленной конфинурацией Ubuntu. После запуска нужно сделать апдейт и установить виртульаное окружение:
```
  sudo apt update
  python3 –version
  sudo apt-get install python3-pip
  sudo apt install python3.10-venv
  python3 -m venv <venv_name>
  source <venv_name>/bin/activate
```
Далее необхожима устновка yandex CLI + авторизация машины для достпуа к проекту с конфигурациями. Детально про создание CLI можно посмотреть [здесь](https://cloud.yandex.ru/ru/docs/cli/operations/install-cli#linux_1), а [здесь](https://cloud.yandex.ru/ru/docs/cli/operations/install-cli#linux_1) иструкция для настройки доступа ВМ к вычислительным ресурсам.

После авторизации нужно установить библиотеку datasphere, а также библиотеки для обучения модели. Все библиотеки собраны в соответсвующем файле requirements.txt
```
sudo python3 -m pip install datasphere
sudo pip install -r requirements.txt
```

Также на ВМ необходимо разместить (все файлы можно посмотреть в этой [папке]():
1. main.py - скрипт для обучения модели и загрузки ее в репозиторий
2. config.yaml - инструкция для запуска задания
3. requirements.txt - список библиотек для работы скрипта. Этот список будет использоваться datasphere для настройки окружения.

Для запуска задания с ВМ нужно добавить в Airflow SSH connection, где в качестве host указывается публичный ip ВМ, с которого будет запускаться задание.

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/67732f45-2452-471b-8975-bc2ab70be73e)

DAG использует SSHOperator, поэтому соответствующий провайдер должен быть установлен в конфигурации. Так как в готовой версии на ВМ этого оператора нет, то я использую версию, развернутую локально из Docker.

</details>
<hr>

### Оценка модели

### Использование модели


## Дальнейшее развитие проекта
