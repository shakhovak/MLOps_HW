{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f935e0a",
   "metadata": {},
   "source": [
    "# Проверяем файлы в кластере и устанавливаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef26965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[K     |████████████████████████████▏   | 278.9 MB 105.0 MB/s eta 0:00:01 |▎                               | 2.7 MB 2.6 MB/s eta 0:02:00| 157.1 MB 115.7 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 316.9 MB 2.6 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 85.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425367 sha256=f2ad5f5a5413f09305c1de3d7e64bd9971d18a6509d80b831ff695a009449dc7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a6/ce/f9/17d82c92f044018df2fe30af63ac043447720d5b2cee39b40f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9056898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebdbb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a5e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 items\n",
      "-rw-r--r--   1 ubuntu hadoop 2807409271 2024-01-14 09:40 /user/hive/warehouse/froms3/2019-08-22.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2854479008 2024-01-14 09:38 /user/hive/warehouse/froms3/2019-09-21.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2895460543 2024-01-14 09:38 /user/hive/warehouse/froms3/2019-10-21.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2939120942 2024-01-14 09:40 /user/hive/warehouse/froms3/2019-11-20.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995462277 2024-01-14 09:38 /user/hive/warehouse/froms3/2019-12-20.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2994906767 2024-01-14 09:37 /user/hive/warehouse/froms3/2020-01-19.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995431240 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-02-18.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995176166 2024-01-14 09:38 /user/hive/warehouse/froms3/2020-03-19.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2996034632 2024-01-14 09:41 /user/hive/warehouse/froms3/2020-04-18.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995666965 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-05-18.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2994699401 2024-01-14 09:41 /user/hive/warehouse/froms3/2020-06-17.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995810010 2024-01-14 09:37 /user/hive/warehouse/froms3/2020-07-17.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995995152 2024-01-14 09:38 /user/hive/warehouse/froms3/2020-08-16.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995778382 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-09-15.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995868596 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-10-15.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995467533 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-11-14.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2994761624 2024-01-14 09:40 /user/hive/warehouse/froms3/2020-12-14.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995390576 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-01-13.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995780517 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-02-12.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995191659 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-03-14.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 2995446495 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-04-13.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3029170975 2024-01-14 09:40 /user/hive/warehouse/froms3/2021-05-13.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042691991 2024-01-14 09:40 /user/hive/warehouse/froms3/2021-06-12.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3041980335 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-07-12.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042662187 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-08-11.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042455173 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-09-10.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042424238 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-10-10.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042358698 2024-01-14 09:41 /user/hive/warehouse/froms3/2021-11-09.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042923985 2024-01-14 09:38 /user/hive/warehouse/froms3/2021-12-09.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042868087 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-01-08.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3043148790 2024-01-14 09:41 /user/hive/warehouse/froms3/2022-02-07.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3042312191 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-03-09.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3041973966 2024-01-14 09:41 /user/hive/warehouse/froms3/2022-04-08.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3073760161 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-05-08.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3089378246 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-06-07.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3089589719 2024-01-14 09:41 /user/hive/warehouse/froms3/2022-07-07.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3090000257 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-08-06.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3089390874 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-09-05.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3109468067 2024-01-14 09:38 /user/hive/warehouse/froms3/2022-10-05.txt\n",
      "-rw-r--r--   1 ubuntu hadoop 3136657969 2024-01-14 09:41 /user/hive/warehouse/froms3/2022-11-04.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs  -ls /user/hive/warehouse/froms3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c2e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/user/hive/warehouse/froms3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c75897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "app_name = \"myApp\" # Ваше имя\n",
    "spark_ui_port = 4040 # Порт для spark ui\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "        .appName(app_name) # имя приложения ? Нужно для отслеживания таски выполнения\n",
    "        .config(\"spark.executor.cores\", \"4\")        \n",
    "        .config(\"spark.executor.memory\", \"4g\") # Executor просее. Ориенир для потребления помаяти. \n",
    "        .config(\"spark.executor.instances\", \"6\")    \n",
    "        .config(\"spark.default.parallelism\", \"48\")\n",
    "        .config(\"spark.driver.memory\", \"4g\") # Main процесс\n",
    "        .config(\"spark.ui.port\", spark_ui_port)\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebef6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"tranaction_id\", IntegerType(), True),\n",
    "    StructField(\"tx_datetime\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"terminal_id\", IntegerType(), True),\n",
    "    StructField(\"tx_amount\", FloatType(), True),\n",
    "    StructField(\"tx_time_seconds\", IntegerType(), True),\n",
    "    StructField(\"tx_time_days\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True)])\n",
    "\n",
    "#data = spark.read.text('/user/hive/warehouse/2019-09-21.txt')\n",
    "\n",
    "df = spark.read.csv(path, \n",
    "                    header=False, schema=schema, comment = \"#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7612641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data overview\n",
      "root\n",
      " |-- tranaction_id: integer (nullable = true)\n",
      " |-- tx_datetime: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- terminal_id: integer (nullable = true)\n",
      " |-- tx_amount: float (nullable = true)\n",
      " |-- tx_time_seconds: integer (nullable = true)\n",
      " |-- tx_time_days: integer (nullable = true)\n",
      " |-- tx_fraud: integer (nullable = true)\n",
      " |-- tx_fraud_scenario: integer (nullable = true)\n",
      "\n",
      "Columns overview\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Data type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tranaction_id</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tx_datetime</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terminal_id</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tx_amount</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tx_time_seconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tx_time_days</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tx_fraud</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tx_fraud_scenario</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column Name  Data type\n",
       "0      tranaction_id        int\n",
       "1        tx_datetime  timestamp\n",
       "2        customer_id        int\n",
       "3        terminal_id        int\n",
       "4          tx_amount      float\n",
       "5    tx_time_seconds        int\n",
       "6       tx_time_days        int\n",
       "7           tx_fraud        int\n",
       "8  tx_fraud_scenario        int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Data overview')\n",
    "df.printSchema()\n",
    "print('Columns overview')\n",
    "pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/user/hive/warehouse/output_preliminary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbac2d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxrwxrwx   - ubuntu hadoop          0 2024-01-14 09:41 /user/hive/warehouse/froms3\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-01-14 10:16 /user/hive/warehouse/output_preliminary.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs  -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdbf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = spark.read.parquet('/user/hive/warehouse/output_preliminary.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67932c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tranaction_id</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>9.398958918339887E8</td>\n",
       "      <td>5.42649141382195E8</td>\n",
       "      <td>0</td>\n",
       "      <td>1879791584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>500416.7652254882</td>\n",
       "      <td>288588.1304047357</td>\n",
       "      <td>-999999</td>\n",
       "      <td>999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terminal_id</th>\n",
       "      <td>1879753826</td>\n",
       "      <td>24933.921642023033</td>\n",
       "      <td>1478655.8994933835</td>\n",
       "      <td>0</td>\n",
       "      <td>89518096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx_amount</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>54.231436693500704</td>\n",
       "      <td>41.286941330176944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16539.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx_time_seconds</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>5.184045645780446E7</td>\n",
       "      <td>2.9929841937784676E7</td>\n",
       "      <td>0</td>\n",
       "      <td>103680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx_time_days</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>599.5075215422339</td>\n",
       "      <td>346.4094747447377</td>\n",
       "      <td>0</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx_fraud</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>0.05922921385341611</td>\n",
       "      <td>0.23605320186213166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx_fraud_scenario</th>\n",
       "      <td>1879794138</td>\n",
       "      <td>0.11930446077388501</td>\n",
       "      <td>0.47748996092105045</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                    1                     2  \\\n",
       "summary                 count                 mean                stddev   \n",
       "tranaction_id      1879794138  9.398958918339887E8    5.42649141382195E8   \n",
       "customer_id        1879794138    500416.7652254882     288588.1304047357   \n",
       "terminal_id        1879753826   24933.921642023033    1478655.8994933835   \n",
       "tx_amount          1879794138   54.231436693500704    41.286941330176944   \n",
       "tx_time_seconds    1879794138  5.184045645780446E7  2.9929841937784676E7   \n",
       "tx_time_days       1879794138    599.5075215422339     346.4094747447377   \n",
       "tx_fraud           1879794138  0.05922921385341611   0.23605320186213166   \n",
       "tx_fraud_scenario  1879794138  0.11930446077388501   0.47748996092105045   \n",
       "\n",
       "                         3           4  \n",
       "summary                min         max  \n",
       "tranaction_id            0  1879791584  \n",
       "customer_id        -999999      999999  \n",
       "terminal_id              0    89518096  \n",
       "tx_amount              0.0    16539.04  \n",
       "tx_time_seconds          0   103680000  \n",
       "tx_time_days             0        1199  \n",
       "tx_fraud                 0           1  \n",
       "tx_fraud_scenario        0           3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbab04",
   "metadata": {},
   "source": [
    "Потенциальные проблемы в датасете:\n",
    "\n",
    "1. могут быть пустые  или нулевые значения (например, по terminal_id - кол-во записей меньше; по tx_amount минимальное значение 0)\n",
    "2. могут быть дубликаты (например, номер транзакции должен быть уникальный)\n",
    "3. могут быть непонятные и неправильные id (например, очень странно видеть id клиента отрицательным; также имеет смысл проверить terminal_id - действительно он может быть таким)\n",
    "4. проверить, что tx_fraud либо 0, либо 1 и что tx_fraud_scenario из списка [0,1,2,3]\n",
    "\n",
    "**Комментарий**: не совсем понятна сущность фичей tx_time_seconds и tx_time_days, поэтому в рамках данного анализа их рассматривать не буду."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39b472",
   "metadata": {},
   "source": [
    "# 1. Проверка на пустые и нулевые значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97050d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",
    ", isnan, udf, hour, array_min, array_max, countDistinct, max, min\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599b6316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tranaction_id</th>\n",
       "      <th>tx_datetime</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>terminal_id</th>\n",
       "      <th>tx_amount</th>\n",
       "      <th>tx_time_seconds</th>\n",
       "      <th>tx_time_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3805</td>\n",
       "      <td>0</td>\n",
       "      <td>40312</td>\n",
       "      <td>35260</td>\n",
       "      <td>3</td>\n",
       "      <td>1565894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tranaction_id  tx_datetime  customer_id  terminal_id  tx_amount  \\\n",
       "0              0         3805            0        40312      35260   \n",
       "\n",
       "   tx_time_seconds  tx_time_days  \n",
       "0                3       1565894  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "# Предположим, что 0 может быть id как клиента, так и терминала\n",
    "string_columns = ['tranaction_id', 'customer_id', 'terminal_id', 'tx_datetime']\n",
    "not_to_review = ['tx_fraud', 'tx_fraud_scenario']\n",
    "\n",
    "missing_values = {} \n",
    "for index, column in enumerate(df_p.columns):\n",
    "    if column in string_columns:    # check string columns with None and Null values\n",
    "        missing_count = df_p.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()\n",
    "        missing_values.update({column:missing_count})\n",
    "    elif column not in not_to_review:\n",
    "        missing_count = df_p.where(col(column).isin([0.0, None, np.nan])).count()\n",
    "        missing_values.update({column:missing_count})\n",
    "\n",
    "missing_df = pd.DataFrame.from_dict([missing_values])\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad74ad",
   "metadata": {},
   "source": [
    "## Вывод:\n",
    "В некторых столбцах есть пропуски и нулевые значения, будем исключать такие строки при предобработке данных. Комментарий по поводу суммы транзакции - будем считать, что если сумма нулевая, то она ошибочна и не будет включаться в анализ. Очевидно, что здесь нужно больше данных для более точного решения.\n",
    "Общее кол-во таких транзакций не значительно и их удаление без корректировки не сильно повлияет на обучение моделей в будущем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3d3f1",
   "metadata": {},
   "source": [
    "# 2. Проверка на дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa113fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2553"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated = df_p.count() - df_p.select('tranaction_id').distinct().count()\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1638d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared unique ids = 1879791585 with sequences used = 1879791584\n"
     ]
    }
   ],
   "source": [
    "ids_used = df_p.select(max(df_p.tranaction_id).alias('max_value')).first().max_value - df_p.select(min(df_p.tranaction_id).alias('min_value')).first().min_value\n",
    "unique_ids = df_p.select('tranaction_id').distinct().count()\n",
    "print(f'Compared unique ids = {unique_ids} with sequences used = {ids_used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1748aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_used - unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc228273",
   "metadata": {},
   "source": [
    "## Выводы.\n",
    "\n",
    "Итак, есть дубликаты ids для транзакций. Так как все-таки значения должны быть уникальными, то при обработке нужно дубликаты удалить. Отмечу, что таких дубликатов немного и удаление не повлияет на обучение моделей в будущем. \n",
    "Проверка на последовательность используемых ids показала, что не все id используются последовательно. Так как разница не совпадает с кол-вом дублиактов, то некторые id не используются. На данном примере оставлю это вариант, в реальном кейсе я бы посмотрела, почему некторые id выпали. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc749c1",
   "metadata": {},
   "source": [
    "# 3. Проверка на правильность id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78ec432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head for customer_id:\n",
      "[Row(customer_id=-999999), Row(customer_id=0), Row(customer_id=1), Row(customer_id=2), Row(customer_id=3), Row(customer_id=4), Row(customer_id=5), Row(customer_id=6), Row(customer_id=7), Row(customer_id=8)]\n",
      "Tail for customer_id:\n",
      "[Row(customer_id=999990), Row(customer_id=999991), Row(customer_id=999992), Row(customer_id=999993), Row(customer_id=999994), Row(customer_id=999995), Row(customer_id=999996), Row(customer_id=999997), Row(customer_id=999998), Row(customer_id=999999)]\n",
      "===================================\n",
      "Head for terminal_id:\n",
      "[Row(terminal_id=None), Row(terminal_id=0), Row(terminal_id=1), Row(terminal_id=2), Row(terminal_id=3), Row(terminal_id=4), Row(terminal_id=5), Row(terminal_id=6), Row(terminal_id=7), Row(terminal_id=8)]\n",
      "Tail for terminal_id:\n",
      "[Row(terminal_id=997), Row(terminal_id=998), Row(terminal_id=999), Row(terminal_id=21861), Row(terminal_id=22129), Row(terminal_id=22225), Row(terminal_id=360740), Row(terminal_id=436002), Row(terminal_id=618230), Row(terminal_id=89518096)]\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "to_review = ['customer_id', 'terminal_id']\n",
    "for item in to_review:\n",
    "    sort = df_p.select(item).distinct().orderBy(item)\n",
    "    print(f'Head for {item}:\\n{sort.head(10)}')\n",
    "    print(f'Tail for {item}:\\n{sort.tail(10)}')\n",
    "    print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c707ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14623"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_customer_ids = (\n",
    "    df_p\n",
    "    .filter(df_p['customer_id'] < 0)\n",
    ")\n",
    "strange_customer_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea326ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575157"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_terminal_ids = (\n",
    "    df_p\n",
    "    .filter(df_p['terminal_id'] > 999)\n",
    ")\n",
    "strange_terminal_ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfce2f",
   "metadata": {},
   "source": [
    "## Выводы.\n",
    "Проверка показала, что нужно будет удалить строки с отрицательным customer_id, так как это скорее всего ошибки. Кол-во таких строк незначительно, поэтому легче удалить, чем исследовать причину ошибки. Хотя в реальной ситуации, я бы провела мини расследования для понимания причины. Может быть так обозначаются ушедшие клиенты и т.д.\n",
    "\n",
    "Также удалить строки, где terminal_id > 999, так как это скорее всего ошибка. Здесь кол-во записей с такими странными номерами больше, лучше бы провести небольшое расследование. Для учебного примера будем удалять такие строки.\n",
    "\n",
    "# 4. Проверка на значения tx_fraud и tx_fraud_scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc272726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_fraud</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>111338729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1768455409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tx_fraud       count\n",
       "0         1   111338729\n",
       "1         0  1768455409"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_count = df_p.groupBy('tx_fraud').count().toPandas()\n",
    "outcome_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb3bc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_fraud_scenario</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1018093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2608461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>107712175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1768455409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tx_fraud_scenario       count\n",
       "0                  1     1018093\n",
       "1                  3     2608461\n",
       "2                  2   107712175\n",
       "3                  0  1768455409"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_count = df_p.groupBy('tx_fraud_scenario').count().toPandas()\n",
    "scenario_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ab50e",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "Итак, обе фичи не содержат ошибок и не требуют дополнительной проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d412ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
