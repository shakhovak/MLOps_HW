# ДЗ № 3
## 1. Создать служебный аккаунт в Yandex Cloud для работы с кластером.

## Решение:
Аккуаунт с необходимыми функциями был создан еще в предыдущем задании. В качестве основных функций для работы с кластером необходимо указать ```dataproc.agent``` и ```mdb.dataproc.agent```. Дополнительно добавила права на чтение и запись в созданном бакете для чтения и хранения данных, чтобы можно было запустить автоматическое исполнение скрипта. 

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/6bf6edb3-ce66-4eaf-a7bb-aaaa1ec2a019)

## 2. Создать новый bucket в Yandex Cloud Object Storage
Предоставить созданному выше системному аккаунту право на запись к нему. Для проверки преподавателем данный bucket необходимо сделать общедоступным на чтение, а точку доступа к нему привести в README-файле Вашего GitHub репозитория.

## Решение:
Буду использовать бакет с публичным доступом создан из UI Yandex Cloud Object Storage s3://mlopsshakhova , созданный в рамках прошлого задания. В этом бакете уже есть скопированные данные.

## 3. Создать Spark-кластер в Data Proc
а) Мастер-подкластер: класс хоста s3-c2-m8, размер хранилища 40 ГБ.
б) Compute-подкластер: класс хоста s3-c4-m16, от 3 хостов, размер хранилища – от 128 ГБ. Требуемый объем локальных дисков зависит от объема кэшированных DataFrames в процессе работы Вашего скрипта.

## Решение:
Спарк-кластер создан:

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/ff61eede-0dd7-4b07-8a72-0b0a9596bb91)

## 4. Проанализировать датасет мошеннических транзакций на наличие в нем ошибочных данных.

Данные из бакета скопированы в hdfs как предлагалось в предыдущем задании. Данные проанализированы в ноутбуке [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/HW_3/data_review.ipynb), запущенном также в кластере при помощи команды ```jupyter notebook --no-browser --port 8888 --ip=*```.

Детальный анализ и выводы можно также найти в этом ноутбуке.
По итогам анализа принято решение провести следующие корретировки в данных:
- удалить пустые значения в данных (в любой из фичей)
- удалить строки, где сумма транзакции равна 0
- удалить дубликаты по транзакциям с одинаковым transaction_id
- удалить строки, где есть отрицательный id клиента
- удалить строки, где id терминала больше 999

## 5. Cоздать скрипт, который должен выполнять очистку данных с использованием Apache Spark.
С учетом предложенных корректировок создан Py-файл, который можно посмотреть [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/HW_3/pyspark_script.py).

## 6.Выполнить очистку датасета с использованием созданного скрипта и сохранить его в созданном выше bucket’е в формате parquet
Для работы скрипта создам задание в кластере:

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/de29aa64-3051-476d-932b-887b47a50f5e)

Обращаю внимание, что задание у меня пока не совсем корректное, так как для ускорения я решила воспользоваться созданным на предыдущем шаге паркетным файлом исходных данных и чистить данные уже из него, а не читать все файлы из бакета s3. При дальнейшем повторении задания я исправлю на обработку исходных файлов.

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/aee62728-71c7-4f22-838b-5134519fe1f3)

Задание отработало успешно, как видно из принт-скрина и обработанные файлы сохранились в указанный бакет.
Для проверки я прочитала данный файл в ноутбуке [здесь](https://github.com/shakhovak/MLOps_HW/blob/master/HW_3/check_file_after_preprocess.ipynb). Ниже привожу итоговый скрин, который демонстрирует изменения в исходных данных.

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/e93e8728-6cfc-4ce8-8ff6-e6bfceed2e3a)







