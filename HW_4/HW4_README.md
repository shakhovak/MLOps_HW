# ДЗ № 4
**Цель работы**: В данном домашнем задании Вы потренируетесь в организации периодического запуска процедуры очистки данных с помощью инструмента Apache Airflow, познакомитесь с концепцией ориентированных
направленных графов (DAGs), с помощью которых организуется последовательность запуска задач по расписанию, научитесь разрабатывать собственные графы с помощью языка Python для Apache Airflow.

## Общий подход
Для решения этой задачи воспользуюсь встроенным в коробоку Airflow провайдером yandex. Этот провайдер содержит оператоторы, которые позволяют сделать следующие шага:

1. Создать временный кластер
2. Запустить на этом кластере job 
3. Удалить временный кластер

Для управления DAG я создала сервисный аккаунт, которому добавила роли, нкобходимые для выполнения описанных выше задач.
![Alt text](image.png)

Для подключения это аккаунта в Airflow созданы 2 ключа:
1. Для s3 статический ключ
2. Для всего сервисного аккаунта - авторизованный ключ, который позволит Airflow создавать и удалять кластеры + jobs

## 1. Запустить Airflow и создать подключения
Запускать Airflow я буду на ВМ. Так как задание я собираюсm выполнять с помощью провайдера Yandex, уже встроенного в готовый Airflow на ВМ, то в качестве подключений я буду использовать 2 основных:
1. для подключения r бакетам s3 будут использоваться статисеские ключи, сгенерированные для сервисного аккаунта
![Alt text](image-1.png)
2. для подключения сервисного аккаунта будет использоваться аторизованный ключ в формате json, который я сохранила вместе с DAG
![Alt text](image-2.png)

## 2. Создать DAG для запуска скрипта по очистке датасет
Для работы со скриптом, который сохранен в бакете Object Storage подготовлен DAG data_proc.py.

![image](https://github.com/shakhovak/MLOps_HW/assets/89096305/cf83f1d7-9532-4d6c-92b8-4e938036ff16)

В нем использованы 3 оператора от провайдера yandex:
1. ```DataprocCreateClusterOperator``` , который позволяет создавать кластер HDFS в Data Proc. Детально можно почитать [здесь](https://airflow.apache.org/docs/apache-airflow-providers-yandex/2.2.0/_api/airflow/providers/yandex/operators/yandexcloud_dataproc/index.html)
2. ```DataprocCreatePysparkJobOperator```, который создает и запускает job на имеющемся в каталоге кластере. Реализовано на основе spark-submit. В качестве скрипта для job используется скрипт по очистке данных, который можно посмотреть вот здесб.
3. ```DataprocDeleteClusterOperator```, который удаляет созданный кластер после завершения job.
